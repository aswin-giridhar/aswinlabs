{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2020-10-22-English-to-Dravidian-Languages-Translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW6wN6Jx_m14"
      },
      "source": [
        "# \"English to Dravidian languages translation\"\n",
        "> \"Create a translation engine to translate from English to 4 Dravidian languages namely Tamil, Telugu, Kannada and Malayalam using Huggingface models.\"\n",
        "\n",
        "- toc: false\n",
        "- branch: master\n",
        "- badges: false\n",
        "- comments: true\n",
        "- categories: [translation, NLP]\n",
        "- image: images/375px-India_South_India_Locator_Map.png\n",
        "- hide: false\n",
        "- search_exclude: true\n",
        "- metadata_key1: metadata_value1\n",
        "- metadata_key2: metadata_value2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYHghULp_6HO"
      },
      "source": [
        "## Intro - Hugging Face\n",
        "\n",
        "Hugging Face is an extremely popular python library which provides state of the art models for various NLP tasks like text classification, machine translation etc. Its enables us to quickly experiment with various NLP architecture using its modules, thereby helping us to focus more on research instead of focusing on the nitty-gritty stuff. \n",
        "\n",
        "One other big plus point is that it supports both Pytorch and Tensorflow frameworks. We can easily switch between the two. And we can also convert it into the ONNX frameword if need for inference.\n",
        "\n",
        "Hugging Face has released various translation models, which you can explore in this [link](https://huggingface.co/models?filter=translation). We would be using the MarianMT [model](https://huggingface.co/Helsinki-NLP/opus-mt-en-dra) which has already been trained on parallel texts involving english and the dravidian languages. MarianMT models main ideas are based out of the [MarianNMT project](https://marian-nmt.github.io/) which mainly used C++. All models the MarinMT models at hugging face are transformer encoder-decoders with 6 layers in each component.\n",
        "\n",
        "# Intro - Translation\n",
        "\n",
        "Machine Translation can be thought of a seq2seq generation task which contains encoder and decoder blocks. To train the model, the encoder receives the sentences in the source language and the decoder is made to predict the sentences in the target languages. You can check out this initial [paper](http://arxiv.org/abs/1609.08144) from Google for more information how it is done.\n",
        "\n",
        "Here in this article we would be using translation models trained on Transformer architecture and you can see how easy it is to create a translation pipleline using the hugging face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4b-J3cOFjcA"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P8WF5UFGR0b",
        "outputId": "bd98ac4c-38f7-4f17-d073-1a7de6e65893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "# Please install the reqired packages before proceeding\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkaBy0TxFbog",
        "outputId": "bf5968ff-3d4d-4fa9-c223-28bf6d7fc610",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Let's start by importing the packages from hugging face essential for this work.\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer # imports the MarianMT model architecture and the tokenizer\n",
        "\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-dra' # This model has been trained on the parallel texts of english and the dravidian languages.\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "print(tokenizer.supported_language_codes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['>>tel<<', '>>kan<<', '>>mal<<', '>>tam<<']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyaTqHafGlG6"
      },
      "source": [
        "Once you run the above code block you can see that the required tokenizer and model is getting downloaded from the hugging face model repository. The print statement prints out the languages supported by the translation engine. Since we are translating from English to the Dravidian languages we can see the 4 language codes of the dravidian languages. \n",
        "\n",
        "All the 4 language codes which you see on the output cell are based out of the \"ISO 639-2\" which is a three letter language classification system. There is also a two letter language classification system which is commonly used called ISO 639-1. You can learn more the different language codes from this [wikipedia link](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes), which has a nice list of all the language codes in various standards.\n",
        "\n",
        "Now let's prepare some texts for the translation engine to translate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n44BxbJJJ81J"
      },
      "source": [
        "text_to_be_translated = ['>>tam<< How are you doing?',\n",
        "                         '>>kan<< How are you doing?',\n",
        "                         '>>tel<< How are you doing?',\n",
        "                         '>>mal<< How are you doing?']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BTHmYyIKOWZ"
      },
      "source": [
        "You can see that I am creating a list of same sentence for the model to translate but I am prepending the language codes of the Dravidian languages in the brackets. This addition of language codes at the beginning of the text is necessary because the translation model which has been trained to predict on mulitple target languages with the source language as English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHkbFiBwGgep",
        "outputId": "55d1002f-4aeb-4712-a23a-92d5d3828149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# The below step creates a batch of text for inferencing after the sentences have tokenized\n",
        "batch_text = tokenizer.prepare_seq2seq_batch(text_to_be_translated)\n",
        "print(batch_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[ 14, 129,  43,  24, 713,  15,   0],\n",
            "        [ 12, 129,  43,  24, 713,  15,   0],\n",
            "        [ 11, 129,  43,  24, 713,  15,   0],\n",
            "        [ 13, 129,  43,  24, 713,  15,   0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VondpJLeL_Bm"
      },
      "source": [
        "In the print statement above you can observe that only the token for the language codes are different after tokenization while the other tokens are same in the input ids for all the sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsqfR3DKL3IX",
        "outputId": "bda7f880-df5e-471f-a127-4fbd7393711f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "translated = model.generate(**batch_text)\n",
        "print(translated)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[62951,  1796,  1381,  4547,  1629,    15,     0],\n",
            "        [62951,   383, 13504,  9075,    15,     0, 62951],\n",
            "        [62951,   934,   230,  6063,    15,     0, 62951],\n",
            "        [62951,  6302, 11736,    15,     0, 62951, 62951]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTzZW_g2OG8Y"
      },
      "source": [
        "This step is used to make the model generate the intermediate representations for the input vectors. The ids which you see in the tensor all have relevant mappings to tokens in the target language. The tokenization technique used here is based on Sentence piece tokenization which tokenizes word to subword and creates a maping dictionary. You can learn more on Sentencepiece tokenization technique in this [paper](https://arxiv.org/pdf/1808.06226.pdf). \n",
        "\n",
        "Now let's explore what do some of the ids in the intermediate representation tensor have as the associated word component for the sentence translated to  Tamil.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSCm6NTGOIQL",
        "outputId": "b7fa6e68-b6ca-41e6-b246-6b03abde1c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "print(\"Word for id 62951:\", tokenizer.decode(token_ids=[62951]))\n",
        "print(\"Word for id 1796:\", tokenizer.decode(token_ids=[1796]))\n",
        "print(\"Word for id 1381:\", tokenizer.decode(token_ids=[1381]))\n",
        "print(\"Word for id 4547:\", tokenizer.decode(token_ids=[4547]))\n",
        "print(\"Word for id 1629:\", tokenizer.decode(token_ids=[1629]))\n",
        "print(\"Word for id 15:\", tokenizer.decode(token_ids=[15]))\n",
        "print(\"Word for id 0:\", tokenizer.decode(token_ids=[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word for id 62951: <pad>\n",
            "Word for id 1796: நீ\n",
            "Word for id 1381: எப்படி\n",
            "Word for id 4547: இருக்கிற\n",
            "Word for id 1629: ாய்\n",
            "Word for id 15: ?\n",
            "Word for id 0: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7paFNRLQHbz"
      },
      "source": [
        "We can observe that 62951, 15 and 0 are the token_ids for \\<PAD>, ? and \"\" respectively. And since the model has been trained on parallel text for all the 4 languages combined, the these ids have similar tokens irrespective of the target language.\n",
        "\n",
        "You would also have observed, if you know Tamil language that the token for ids 4547 and 1629 from a single word but are split into two subwords because of the sentencepiece tokenizer.\n",
        "\n",
        "Now let's decode the list of sentences in the tensor using the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2KL8k-TL3Mj",
        "outputId": "3efcf69c-f0b9-42ff-80e0-49769eae95ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
        "print(tgt_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['நீ எப்படி இருக்கிறாய்?', 'ನೀವು ಹೇಗಿದ್ದೀರಿ?', 'ఎలా మీరు చేస్తున్న?', 'സുഖമാണോ?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWpg-3DDSjaT"
      },
      "source": [
        "So you have now created a setup of English to Dravidian languages translation in less than 10 steps using the hugging face package. You can also implement this translation activity using the pipeline feature of hugging face which  abstracts the entire process. So let's take a look at how that works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_uicXjYM-uK",
        "outputId": "5a423953-83ce-4554-b848-862128b50ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from transformers import pipeline, MarianTokenizer, MarianMTModel\n",
        "\n",
        "model_name = 'Helsinki-NLP/opus-mt-en-dra' # This model has been trained on the parallel texts of english and the dravidian languages.\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "translation_engine = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "text_to_translate = input(prompt='Please enter the English text to translate:\\n')\n",
        "\n",
        "lang_select = input(prompt='Please enter one of the following languages: 1) Tamil, 2) Telugu, 3) Kannada and 4) Malayalam:\\n')\n",
        "if lang_select == \"Tamil\":\n",
        "  text_to_translate = \">>tam<<\" + text_to_translate \n",
        "elif lang_select == \"Kannada\":\n",
        "  text_to_translate = \">>kan<<\" + text_to_translate \n",
        "elif lang_select == \"Telugu\":\n",
        "  text_to_translate = \">>tel<<\" + text_to_translate \n",
        "elif lang_select == \"Malayalam\":\n",
        "  text_to_translate = \">>mal<<\" + text_to_translate \n",
        "\n",
        "translated_text = translation_engine(text_to_translate)\n",
        "print(\"The translated text is: {}\".format(translated_text[0][\"generated_text\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the English text to translate:\n",
            "hello, how are you doing?\n",
            "Please enter one of the following languages: 1) Tamil, 2) Telugu, 3) Kannada and 4) Malayalam:\n",
            "Tamil\n",
            "The translated text is: ஹலோ, நீ எப்படி இருக்கிறாய்?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkvbSq4PbS1G"
      },
      "source": [
        "As you can see this abstracts the majority of the technical know-hows and creates easy to use pipeline which would enable us to make products faster."
      ]
    }
  ]
}